# Î¤Î¿Ï€Î¿Î¸Î­Ï„Î·ÏƒÎµ Î±Ï…Ï„Î­Ï‚ Ï„Î¹Ï‚ Î³ÏÎ±Î¼Î¼Î­Ï‚ Î‘ÎœÎ•Î£Î‘ ÏƒÏ„Î·Î½ ÎºÎ¿ÏÏ…Ï†Î® Ï„Î¿Ï… Î±ÏÏ‡ÎµÎ¯Î¿Ï…
import pysqlite3
import sys
sys.modules["sqlite3"] = sys.modules["pysqlite3"]

# Î£Ï„Î· ÏƒÏ…Î½Î­Ï‡ÎµÎ¹Î±, Î±ÎºÎ¿Î»Î¿Ï…Î¸Î¿ÏÎ½ ÏŒÎ»ÎµÏ‚ Î¿Î¹ Ï…Ï€ÏŒÎ»Î¿Î¹Ï€ÎµÏ‚ ÎµÎ¹ÏƒÎ±Î³Ï‰Î³Î­Ï‚
import streamlit as st
import io
import os
import asyncio
from langchain.prompts import PromptTemplate
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain.chains import ConversationalRetrievalChain
from langchain_core.messages import HumanMessage, AIMessage
from PyPDF2 import PdfReader
from bs4 import BeautifulSoup
import requests
import tiktoken # Î ÏÎ¿ÏƒÎ¸Î­ÏƒÎ±Î¼Îµ Ï„Î· Î²Î¹Î²Î»Î¹Î¿Î¸Î®ÎºÎ· Î³Î¹Î± Ï„Î¿Î½ Ï…Ï€Î¿Î»Î¿Î³Î¹ÏƒÎ¼ÏŒ Ï„Ï‰Î½ tokens

# Î¡ÏÎ¸Î¼Î¹ÏƒÎ· Ï„Î¿Ï… asyncio event loop Î³Î¹Î± Î½Î± Î±Ï€Î¿Ï†ÎµÏ…Ï‡Î¸ÎµÎ¯ Ï„Î¿ Î»Î¬Î¸Î¿Ï‚ "There is no current event loop"
try:
    _ = asyncio.get_running_loop()
except RuntimeError as ex:
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)

# Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Ï„Î¿Ï… Gemini Pro Î¼Î¿Î½Ï„Î­Î»Î¿Ï…
llm = ChatGoogleGenerativeAI(model="gemini-1.5-flash", temperature=0.5, google_api_key=st.secrets["GOOGLE_API_KEY"])

def get_text_from_url(url):
    """Î”Î¹Î±Î²Î¬Î¶ÎµÎ¹ Ï„Î¿ ÎºÎµÎ¯Î¼ÎµÎ½Î¿ Î±Ï€ÏŒ Î¼Î¹Î± Î¹ÏƒÏ„Î¿ÏƒÎµÎ»Î¯Î´Î± Î¼Îµ timeout."""
    try:
        response = requests.get(url, timeout=15) # Î ÏÎ¿ÏƒÎ¸Î®ÎºÎ· timeout 15 Î´ÎµÏ…Ï„ÎµÏÎ¿Î»Î­Ï€Ï„Ï‰Î½
        response.raise_for_status() # Î•Î»Î­Î³Ï‡ÎµÎ¹ Î³Î¹Î± ÏƒÏ†Î¬Î»Î¼Î±Ï„Î± HTTP
        
        soup = BeautifulSoup(response.text, 'html.parser')
        # Î‘Ï†Î±Î¹ÏÎ¿ÏÎ¼Îµ Ï„Î± scripts, styles ÎºÎ»Ï€ Î³Î¹Î± Î½Î± Î­Ï‡Î¿Ï…Î¼Îµ ÎºÎ±Î¸Î±ÏÏŒ ÎºÎµÎ¯Î¼ÎµÎ½Î¿
        for script in soup(["script", "style", "header", "footer", "nav"]):
            script.decompose()
        
        text = soup.get_text()
        
        # Î ÏÎ¿ÏƒÎ¸Î®ÎºÎ· ÎµÎ»Î­Î³Ï‡Î¿Ï… Î³Î¹Î± Ï„Î¿ Î¼Î­Î³ÎµÎ¸Î¿Ï‚ Ï„Î¿Ï… ÎºÎµÎ¹Î¼Î­Î½Î¿Ï…
        MAX_TEXT_LENGTH = 100000 # ÎœÎ­Î³Î¹ÏƒÏ„Î¿ ÏŒÏÎ¹Î¿ Ï‡Î±ÏÎ±ÎºÏ„Î®ÏÏ‰Î½ Î³Î¹Î± Î±Ï€Î¿Ï†Ï…Î³Î® Ï…Ï€ÎµÏÏ†ÏŒÏÏ„Ï‰ÏƒÎ·Ï‚
        if len(text) > MAX_TEXT_LENGTH:
            st.warning(f"Î¤Î¿ ÎºÎµÎ¯Î¼ÎµÎ½Î¿ Î±Ï€ÏŒ Ï„Î¿ URL {url} ÎµÎ¯Î½Î±Î¹ Ï€Î¿Î»Ï Î¼ÎµÎ³Î¬Î»Î¿. Î˜Î± ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÏ„Ï Î¼ÏŒÎ½Î¿ Ï„Î± Ï€ÏÏÏ„Î± {MAX_TEXT_LENGTH} bytes.")
            text = text[:MAX_TEXT_LENGTH]
            
        return text
    except requests.exceptions.RequestException as e:
        st.error(f"Î£Ï†Î¬Î»Î¼Î± ÎºÎ±Ï„Î¬ Ï„Î·Î½ Î±Î½Î¬Î³Î½Ï‰ÏƒÎ· Ï„Î¿Ï… URL {url}: {e}. Î•Î»Î­Î³Î¾Ï„Îµ Î±Î½ Ï„Î¿ URL ÎµÎ¯Î½Î±Î¹ Î­Î³ÎºÏ…ÏÎ¿ ÎºÎ±Î¹ Ï€ÏÎ¿ÏƒÎ²Î¬ÏƒÎ¹Î¼Î¿.")
        return ""
    except Exception as e:
        st.error(f"Î£Ï†Î¬Î»Î¼Î± ÎºÎ±Ï„Î¬ Ï„Î·Î½ ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î± Ï„Î¿Ï… Ï€ÎµÏÎ¹ÎµÏ‡Î¿Î¼Î­Î½Î¿Ï… Ï„Î¿Ï… URL {url}: {e}")
        return ""


@st.cache_resource
def process_documents(pdf_directory):
    """Î•Ï€ÎµÎ¾ÎµÏÎ³Î¬Î¶ÎµÏ„Î±Î¹ Ï„Î± PDF, HTML ÎºÎ±Î¹ URLs Ï€Î¿Ï… Î²ÏÎ¯ÏƒÎºÎ¿Î½Ï„Î±Î¹ ÏƒÏ„Î¿Î½ Ï†Î¬ÎºÎµÎ»Î¿ Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½."""
    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len
    )
    all_text = ""

    # Î’ÏÎ¯ÏƒÎºÎµÎ¹ ÏŒÎ»Î± Ï„Î± Î±ÏÏ‡ÎµÎ¯Î± PDF, HTML ÎºÎ±Î¹ Ï„Î¿ urls.txt ÏƒÏ„Î¿Î½ Ï†Î¬ÎºÎµÎ»Î¿
    files = [os.path.join(pdf_directory, f) for f in os.listdir(pdf_directory)]

    for file_path in files:
        try:
            if file_path.endswith('.pdf'):
                # Î”Î¹Î±Î²Î¬Î¶ÎµÎ¹ PDF
                pdf_reader = PdfReader(file_path)
                for page in pdf_reader.pages:
                    all_text += page.extract_text()
            elif file_path.endswith(('.html', '.htm')):
                # Î”Î¹Î±Î²Î¬Î¶ÎµÎ¹ HTML
                with open(file_path, 'r', encoding='utf-8') as file:
                    html_content = file.read()
                    soup = BeautifulSoup(html_content, 'html.parser')
                    all_text += soup.get_text()
            elif file_path.endswith('.txt') and os.path.basename(file_path) == 'urls.txt':
                # Î”Î¹Î±Î²Î¬Î¶ÎµÎ¹ URLs Î±Ï€ÏŒ Ï„Î¿ Î±ÏÏ‡ÎµÎ¯Î¿ urls.txt
                with open(file_path, 'r', encoding='utf-8') as file:
                    urls = file.read().splitlines()
                    for url in urls:
                        if url.strip():
                            all_text += get_text_from_url(url.strip())

        except Exception as e:
            st.error(f"Î£Ï†Î¬Î»Î¼Î± ÎºÎ±Ï„Î¬ Ï„Î·Î½ Î±Î½Î¬Î³Î½Ï‰ÏƒÎ· Ï„Î¿Ï… Î±ÏÏ‡ÎµÎ¯Î¿Ï… {file_path}: {e}")
            return None
    
    if not all_text.strip():
        st.error("Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎµ ÎºÎµÎ¯Î¼ÎµÎ½Î¿ Î³Î¹Î± ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î±. Î•Î»Î­Î³Î¾Ï„Îµ Ï„Î± Î±ÏÏ‡ÎµÎ¯Î± Î® Ï„Î¿ URL.")
        return None

    text_chunks = text_splitter.split_text(all_text)
    embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
    vector_store = Chroma.from_texts(text_chunks, embeddings)
    qa_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=vector_store.as_retriever(),
        return_source_documents=True
    )
    return qa_chain

# Î¡ÏÎ¸Î¼Î¹ÏƒÎ· Ï„Î¿Ï… Streamlit UI
st.set_page_config(page_title="Î‘Ï…Ï„ÏŒÎ½Î¿Î¼Î¿ RAG Chatbot", layout="wide")
st.header("ğŸ’¬ Î‘Ï…Ï„ÏŒÎ½Î¿Î¼Î¿ RAG Chatbot Î¼Îµ Gemini")

# Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Î¹ÏƒÏ„Î¿ÏÎ¹ÎºÎ¿Ï ÏƒÏ…Î½Î¿Î¼Î¹Î»Î¯Î±Ï‚
if "messages" not in st.session_state:
    st.session_state.messages = []
if "qa_chain" not in st.session_state:
    st.session_state.qa_chain = None

# Î”Î·Î¼Î¹Î¿Ï…ÏÎ³Î¯Î± Ï„Î·Ï‚ Î´Î¹Î±Î´ÏÎ¿Î¼Î®Ï‚ Ï€ÏÎ¿Ï‚ Ï„Î¿Î½ Ï†Î¬ÎºÎµÎ»Î¿ Î¼Îµ Ï„Î± Î­Î³Î³ÏÎ±Ï†Î±
pdf_dir = "data"

# Î•Î¼Ï†Î¬Î½Î¹ÏƒÎ· Î¼Î·Î½ÏÎ¼Î±Ï„Î¿Ï‚ Ï†ÏŒÏÏ„Ï‰ÏƒÎ·Ï‚ ÏƒÏ„Î·Î½ Î±ÏÏ‡Î®
if st.session_state.qa_chain is None:
    with st.spinner("Î•Ï€ÎµÎ¾ÎµÏÎ³Î¬Î¶Î¿Î¼Î±Î¹ Ï„Î± Î­Î³Î³ÏÎ±Ï†Î± ÎºÎ±Î¹ Ï„Î± URLs..."):
        st.session_state.qa_chain = process_documents(pdf_dir)
        if st.session_state.qa_chain:
            st.success("Î¤Î± Î­Î³Î³ÏÎ±Ï†Î± ÎµÏ€ÎµÎ¾ÎµÏÎ³Î¬ÏƒÏ„Î·ÎºÎ±Î½ Î¼Îµ ÎµÏ€Î¹Ï„Ï…Ï‡Î¯Î±!")
        else:
            st.error("Î‘Î´Ï…Î½Î±Î¼Î¯Î± ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î±Ï‚ Ï„Ï‰Î½ ÎµÎ³Î³ÏÎ¬Ï†Ï‰Î½. Î Î±ÏÎ±ÎºÎ±Î»Ï ÎµÎ»Î­Î³Î¾Ï„Îµ Ï„Î± Î±ÏÏ‡ÎµÎ¯Î± ÏƒÎ±Ï‚.")

# Î•Î¼Ï†Î¬Î½Î¹ÏƒÎ· Ï„Î¿Ï… Î¹ÏƒÏ„Î¿ÏÎ¹ÎºÎ¿Ï ÏƒÏ…Î½Î¿Î¼Î¹Î»Î¯Î±Ï‚
for message in st.session_state.messages:
    with st.chat_message(message["role"]):
        st.markdown(message["content"])

# Î•Î¯ÏƒÎ¿Î´Î¿Ï‚ Ï‡ÏÎ®ÏƒÏ„Î·
if prompt := st.chat_input("Î¡ÏÏ„Î·ÏƒÎ­ Î¼Îµ ÎºÎ¬Ï„Î¹ Î³Î¹Î± Ï„Î± Î­Î³Î³ÏÎ±Ï†Î± Î® Ï„Î¹Ï‚ Î¹ÏƒÏ„Î¿ÏƒÎµÎ»Î¯Î´ÎµÏ‚..."):
    # Î•Î¼Ï†Î¬Î½Î¹ÏƒÎ· ÎµÏÏÏ„Î·ÏƒÎ·Ï‚ Ï‡ÏÎ®ÏƒÏ„Î·
    st.session_state.messages.append({"role": "user", "content": prompt})
    with st.chat_message("user"):
        st.markdown(prompt)

    # Î Î±ÏÎ±Î³Ï‰Î³Î® Î±Ï€Î¬Î½Ï„Î·ÏƒÎ·Ï‚ Î±Ï€ÏŒ Ï„Î¿ chatbot
    with st.chat_message("assistant"):
        if st.session_state.qa_chain:
            # ÎœÎµÏ„Î±Ï„ÏÎ¿Ï€Î® Î¹ÏƒÏ„Î¿ÏÎ¹ÎºÎ¿Ï ÏƒÎµ ÏƒÏ…Î¼Î²Î±Ï„Î® Î¼Î¿ÏÏ†Î®
            chat_history_formatted = []
            for msg in st.session_state.messages:
                if msg["role"] == "user":
                    chat_history_formatted.append(HumanMessage(content=msg["content"]))
                else:
                    chat_history_formatted.append(AIMessage(content=msg["content"]))

            response = st.session_state.qa_chain({"question": prompt, "chat_history": chat_history_formatted})
            answer = response["answer"]
            st.markdown(answer)
        else:
            answer = "Î Î±ÏÎ±ÎºÎ±Î»Ï ÎµÏ€Î±Î½ÎµÎºÎºÎ¹Î½Î®ÏƒÏ„Îµ Ï„Î·Î½ ÎµÏ†Î±ÏÎ¼Î¿Î³Î® Î³Î¹Î± Î½Î± ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÏ„Î¿ÏÎ½ Ï„Î± Î­Î³Î³ÏÎ±Ï†Î±."
            st.markdown(answer)

    # Î‘Ï€Î¿Î¸Î®ÎºÎµÏ…ÏƒÎ· Î±Ï€Î¬Î½Ï„Î·ÏƒÎ·Ï‚ ÏƒÏ„Î¿ Î¹ÏƒÏ„Î¿ÏÎ¹ÎºÏŒ
    st.session_state.messages.append({"role": "assistant", "content": answer})
