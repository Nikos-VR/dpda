import pysqlite3
import sys
sys.modules["sqlite3"] = sys.modules["pysqlite3"]

import streamlit as st
import os
import asyncio
import mysql.connector
from langchain_google_genai import ChatGoogleGenerativeAI
from langchain.text_splitter import RecursiveCharacterTextSplitter
from langchain_chroma import Chroma
from langchain_google_genai import GoogleGenerativeAIEmbeddings
from langchain.chains import ConversationalRetrievalChain
from langchain_core.messages import HumanMessage, AIMessage
from langchain_core.documents import Document

try:
    _ = asyncio.get_running_loop()
except RuntimeError as ex:
    loop = asyncio.new_event_loop()
    asyncio.set_event_loop(loop)

llm = ChatGoogleGenerativeAI(model="gemini-2.5-flash", temperature=0.5, google_api_key=st.secrets["GOOGLE_API_KEY"])

@st.cache_resource
def get_database_connection():
    try:
        conn = mysql.connector.connect(
            host=st.secrets["host"],
            user=st.secrets["user"],
            password=st.secrets["password"],
            database=st.secrets["db"]
        )
        return conn
    except Exception as e:
        st.error(f"Î£Ï†Î¬Î»Î¼Î± ÏƒÏÎ½Î´ÎµÏƒÎ·Ï‚ ÏƒÏ„Î· Î²Î¬ÏƒÎ· Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½: {e}")
        return None

@st.cache_resource
def process_data_from_db(data_from_db):
    st.info("Î•Ï€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î± Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½...")
    
    documents = []
    if not data_from_db:
        st.warning("Î”ÎµÎ½ Î²ÏÎ­Î¸Î·ÎºÎ±Î½ Î´ÎµÎ´Î¿Î¼Î­Î½Î± Î³Î¹Î± ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î±.")
        return None

    for idx, row in enumerate(data_from_db):
        content = " ".join(str(item) for item in row)
        documents.append(Document(page_content=content, metadata={"source": f"Database Row {idx+1}"}))
        
    st.write(f"Î’ÏÎ­Î¸Î·ÎºÎ±Î½ {len(documents)} Î³ÏÎ±Î¼Î¼Î­Ï‚ Î³Î¹Î± ÎµÏ€ÎµÎ¾ÎµÏÎ³Î±ÏƒÎ¯Î±.")

    text_splitter = RecursiveCharacterTextSplitter(
        chunk_size=1000,
        chunk_overlap=200,
        length_function=len
    )
    text_chunks = text_splitter.split_documents(documents)
    
    embeddings = GoogleGenerativeAIEmbeddings(model="models/text-embedding-004")
    vector_store = Chroma.from_documents(text_chunks, embeddings)
    
    qa_chain = ConversationalRetrievalChain.from_llm(
        llm=llm,
        retriever=vector_store.as_retriever(),
        return_source_documents=True
    )
    return qa_chain

st.set_page_config(page_title="MySQL & Gemini Chatbot", layout="wide")
st.header("ğŸ’¬ Chatbot Î¼Îµ Î”ÎµÎ´Î¿Î¼Î­Î½Î± Î±Ï€ÏŒ MySQL")

if "messages" not in st.session_state:
    st.session_state.messages = []
if "qa_chain" not in st.session_state:
    st.session_state.qa_chain = None

if st.session_state.qa_chain is None:
    st.info("Î¦ÏŒÏÏ„Ï‰ÏƒÎ· Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½...")
    conn = get_database_connection()
    if conn:
        try:
            with conn.cursor() as cursor:
                cursor.execute("SELECT * FROM table1;")
                results = cursor.fetchall()
            if results:
                st.session_state.qa_chain = process_data_from_db(results)
                st.success("Î¤Î± Î´ÎµÎ´Î¿Î¼Î­Î½Î± Ï†Î¿ÏÏ„ÏÎ¸Î·ÎºÎ±Î½ Î¼Îµ ÎµÏ€Î¹Ï„Ï…Ï‡Î¯Î±! ÎœÏ€Î¿ÏÎµÎ¯Ï„Îµ Î½Î± Î¾ÎµÎºÎ¹Î½Î®ÏƒÎµÏ„Îµ Ï„Î· ÏƒÏ…Î½Î¿Î¼Î¹Î»Î¯Î±.")
            else:
                st.warning("ÎŸ Ï€Î¯Î½Î±ÎºÎ±Ï‚ ÎµÎ¯Î½Î±Î¹ Î¬Î´ÎµÎ¹Î¿Ï‚ Î® Ï„Î¿ query Î´ÎµÎ½ ÎµÏ€Î­ÏƒÏ„ÏÎµÏˆÎµ Î±Ï€Î¿Ï„ÎµÎ»Î­ÏƒÎ¼Î±Ï„Î±.")
                st.session_state.qa_chain = None
        except Exception as e:
            st.error(f"Î£Ï†Î¬Î»Î¼Î± ÎºÎ±Ï„Î¬ Ï„Î·Î½ ÎµÎºÏ„Î­Î»ÎµÏƒÎ· Ï„Î¿Ï… query: {e}")
            st.session_state.qa_chain = None
    else:
        st.error("Î”ÎµÎ½ Î®Ï„Î±Î½ Î´Ï…Î½Î±Ï„Î® Î· ÏƒÏÎ½Î´ÎµÏƒÎ· ÏƒÏ„Î· Î²Î¬ÏƒÎ· Î´ÎµÎ´Î¿Î¼Î­Î½Ï‰Î½.")

if st.session_state.qa_chain:
    st.write("---")
    
    for message in st.session_state.messages:
        with st.chat_message(message["role"]):
            st.markdown(message["content"])

    if prompt := st.chat_input("Î¡Ï‰Ï„Î®ÏƒÏ„Îµ ÎºÎ¬Ï„Î¹..."):
        st.session_state.messages.append({"role": "user", "content": prompt})
        with st.chat_message("user"):
            st.markdown(prompt)

        with st.chat_message("assistant"):
            last_two_messages = st.session_state.messages[-2:]
            chat_history_formatted = [
                HumanMessage(content=msg["content"]) if msg["role"] == "user" else AIMessage(content=msg["content"])
                for msg in last_two_messages
            ]

            response = st.session_state.qa_chain({"question": prompt, "chat_history": chat_history_formatted})
            answer = response["answer"]
            st.markdown(answer)

        st.session_state.messages.append({"role": "assistant", "content": answer})
